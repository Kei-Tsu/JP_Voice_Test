# ml_model.py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
import joblib

import librosa

def analyze_volume(y, sr):
    """åŸºæœ¬çš„ãªéŸ³é‡åˆ†æã‚’è¡Œã†é–¢æ•°ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰"""
    extractor = VoiceFeatureExtractor()
    features = extractor.extract_features(y, sr)
    return features 

def plot_audio_analysis(features, audio_data, sr):
    """éŸ³å£°åˆ†æã®è¦–è¦šåŒ–ã‚’è¡Œã†é–¢æ•°"""
    # 2ã¤ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆ
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))
    
    # 1ã¤ç›®ã®ãƒ—ãƒ­ãƒƒãƒˆ: æ³¢å½¢è¡¨ç¤º
    librosa.display.waveshow(audio_data, sr=sr, ax=ax1)
    ax1.set_title('Audio Waveform')
    ax1.set_xlabel('Time (Seconds)')
    ax1.set_ylabel('Amplitude')
    
    # 2ã¤ç›®ã®ãƒ—ãƒ­ãƒƒãƒˆ: éŸ³é‡å¤‰åŒ–
    rms = features['rms']
    times = features['times']
    ax2.plot(times, rms, color='blue', label='Volume (RMS)')
    ax2.set_title('Volume Change Over Time')
    ax2.set_xlabel('Time (seconds)')
    ax2.set_ylabel('Volume (RMS)')
    
    # æ–‡æœ«éƒ¨åˆ†ï¼ˆæœ€å¾Œã®20%ï¼‰ã‚’å¼·èª¿è¡¨ç¤º
    if len(times) > 0:
        end_portion = max(1, int(len(times) * 0.2))  # æœ€å¾Œã®20%
        start_highlight = times[-end_portion]
        end_time = times[-1]
        ax2.axvspan(start_highlight, end_time, color='red', alpha=0.2)
        ax2.text(start_highlight + (end_time - start_highlight)/10, 
               max(rms) * 0.8, 'End Part (last 20%)', color='red')
    
    # æ–‡é ­ãƒ»æ–‡ä¸­ãƒ»æ–‡æœ«ã®å¹³å‡éŸ³é‡ã‚’æ°´å¹³ç·šã§è¡¨ç¤º
    ax2.axhline(y=features['start_volume'], color='green', linestyle='--', label='Start Volume Average')
    ax2.axhline(y=features['middle_volume'], color='orange', linestyle='--', label='Middle Volume Average')
    ax2.axhline(y=features['end_volume'], color='red', linestyle='--', label='End Volume Average')
    ax2.legend()
    
    plt.tight_layout()
    return fig

def evaluate_clarity(features):
    """éŸ³é‡ç‰¹å¾´ã‹ã‚‰ã‚¯ãƒªã‚¢ãªç™ºè©±ã‹ã©ã†ã‹ã‚’è©•ä¾¡ã™ã‚‹é–¢æ•°"""
    drop_rate = features["end_drop_rate"]
    last_20_drop_rate = features.get("last_20_percent_drop_rate", 0)  # ã‚­ãƒ¼ãŒãªã„å ´åˆã¯0
    
    # ä¸¡æ–¹ã®ãƒ‰ãƒ­ãƒƒãƒ—ç‡ã‚’è€ƒæ…®ã—ãŸè©•ä¾¡
    avg_drop_rate = (drop_rate + last_20_drop_rate) / 2
    
    if avg_drop_rate < 0.1:
        clarity_level = "è‰¯å¥½"
        advice = "èªå°¾ã¾ã§ã—ã£ã‹ã‚Šç™ºè©±ã§ãã¦ã„ã¾ã™ï¼ãƒãƒ©ãƒ³ã‚¹ãŒã‚ˆã„ç™ºè©±ã§ã™ã€‚"
        score = min(100, int((1 - avg_drop_rate) * 100))
    elif avg_drop_rate < 0.25:
        clarity_level = "æ™®é€š"
        advice = "èªå°¾ãŒã‚„ã‚„å¼±ã¾ã£ã¦ã„ã¾ã™ã€‚ã‚‚ã†å°‘ã—æ–‡æœ«ã‚’æ„è­˜ã™ã‚‹ã¨è‰¯ã„ã§ã—ã‚‡ã†ã€‚"
        score = int(75 - (avg_drop_rate - 0.1) * 100)
    elif avg_drop_rate < 0.4:
        clarity_level = "ã‚„ã‚„å¼±ã„"
        advice = "æ–‡æœ«ã®éŸ³é‡ãŒã‹ãªã‚Šä½ä¸‹ã—ã¦ã„ã¾ã™ã€‚æ–‡æœ«ã‚’1éŸ³ä¸Šã’ã‚‹ã‚¤ãƒ¡ãƒ¼ã‚¸ã§è©±ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
        score = int(60 - (avg_drop_rate - 0.25) * 100)
    else:
        clarity_level = "æ”¹å–„å¿…è¦"
        advice = "èªå°¾ã®éŸ³é‡ãŒå¤§ããä½ä¸‹ã—ã¦ã„ã¾ã™ã€‚æ—¥æœ¬èªã¯æ–‡æœ«ã«é‡è¦ãªæƒ…å ±ã‚„çµè«–ãŒæ¥ã‚‹ã“ã¨ãŒå¤šã„ã§ã™ã€æ–‡æœ«ã¾ã§æ„è­˜ã—ã¦ç›¸æ‰‹ã«ä¼ãˆã‚‹ç·´ç¿’ã‚’ã—ã¾ã—ã‚‡ã†ã€‚"
        score = max(20, int(40 - (avg_drop_rate - 0.4) * 50))
    
    return {
        "clarity_level": clarity_level,
        "advice": advice,
        "score": score,
        "avg_drop_rate": avg_drop_rate
    }

# ãƒ‰ãƒ­ãƒƒãƒ—ç‡ã«å¿œã˜ãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç”Ÿæˆ
def get_feedback(drop_rate):
    if drop_rate < 0.1:
        return {
            "level": "good",
            "message": "è‰¯ã„æ„Ÿã˜ã§ã™ï¼èªå°¾ã¾ã§ã—ã£ã‹ã‚Šç™ºéŸ³ã§ãã¦ã„ã¾ã™ã€‚",
            "emoji": "ğŸŒŸ"
        }
    elif drop_rate < 0.25:
        return {
            "level": "medium",
            "message": "èªå°¾ãŒã‚„ã‚„å¼±ã¾ã£ã¦ã„ã¾ã™ã€‚ã‚‚ã†å°‘ã—æ„è­˜ã—ã¾ã—ã‚‡ã†ã€‚",
            "emoji": "âš ï¸"
        }
    else:
        return {
            "level": "bad",
            "message": "èªå°¾ã®éŸ³é‡ãŒå¤§ããä½ä¸‹ã—ã¦ã„ã¾ã™ã€‚æ–‡æœ«ã‚’æ„è­˜ã—ã¦ï¼",
            "emoji": "â—"
        }
    
def generate_training_data():
    """æ©Ÿæ¢°å­¦ç¿’ç”¨ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°"""
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã¦ä½¿ç”¨ï¼‰
    X = []
    y = []
    
    # ã€Œè‰¯å¥½ã€ãªéŸ³å£°ï¼ˆæ–‡æœ«éŸ³é‡ä½ä¸‹ãŒå°ã•ã„ï¼‰ã®ãƒ‡ãƒ¼ã‚¿
    for _ in range(50):
        features = [
            np.random.uniform(0.1, 0.3),     # mean_volume
            np.random.uniform(0.02, 0.05),   # std_volume
            np.random.uniform(0.1, 0.3),     # start_volume
            np.random.uniform(0.1, 0.3),     # middle_volume
            np.random.uniform(0.09, 0.25),   # end_volumeï¼ˆã‚ã¾ã‚Šä½ä¸‹ã—ãªã„ï¼‰
            np.random.uniform(0.05, 0.15),   # end_drop_rateï¼ˆå°ã•ã„ï¼‰
            np.random.uniform(0.09, 0.25),   # last_20_percent_volume
            np.random.uniform(0.05, 0.15),   # last_20_percent_drop_rate
            np.random.uniform(1000, 2000),   # spectral_centroid_mean
            np.random.uniform(2, 4),         # speech_rate
        ]

        X.append(features)
        y.append("è‰¯å¥½")  # ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«
    
    # ã€Œæ–‡æœ«ãŒå¼±ã„ã€éŸ³å£°ã®ãƒ‡ãƒ¼ã‚¿
    for _ in range(50):
        features = [
            np.random.uniform(0.1, 0.3),     # mean_volume
            np.random.uniform(0.02, 0.05),   # std_volume
            np.random.uniform(0.1, 0.3),     # start_volume
            np.random.uniform(0.1, 0.3),     # middle_volume
            np.random.uniform(0.02, 0.08),   # end_volumeï¼ˆå¤§ããä½ä¸‹ï¼‰
            np.random.uniform(0.3, 0.5),     # end_drop_rateï¼ˆå¤§ãã„ï¼‰
            np.random.uniform(0.02, 0.08),   # last_20_percent_volume
            np.random.uniform(0.3, 0.5),     # last_20_percent_drop_rate
            np.random.uniform(1000, 2000),   # spectral_centroid_mean
            np.random.uniform(2, 4),         # speech_rate
        ]

        X.append(features)
        y.append("æ–‡æœ«ãŒå¼±ã„")  # ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«

    # ã€Œå°å£°ã™ãã‚‹ã€éŸ³å£°ã®ãƒ‡ãƒ¼ã‚¿
    for _ in range(50):
        features = [
            np.random.uniform(0.01, 0.05),   # mean_volumeï¼ˆå…¨ä½“çš„ã«å°ã•ã„ï¼‰
            np.random.uniform(0.01, 0.02),   # std_volume
            np.random.uniform(0.01, 0.05),   # start_volume
            np.random.uniform(0.01, 0.05),   # middle_volume
            np.random.uniform(0.01, 0.03),   # end_volume
            np.random.uniform(0.1, 0.3),     # end_drop_rate
            np.random.uniform(0.01, 0.03),   # last_20_percent_volume
            np.random.uniform(0.1, 0.3),     # last_20_percent_drop_rate
            np.random.uniform(800, 1500),    # spectral_centroid_mean
            np.random.uniform(1, 3),         # speech_rate
        ]
        
        X.append(features)
        y.append("å°å£°ã™ãã‚‹")  # ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«
    
    return np.array(X), np.array(y)  


# éŸ³å£°åˆ†æã®ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã¨é–¢æ•°ã®å®šç¾©
class VoiceFeatureExtractor:
    """éŸ³å£°ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def extract_features(self, audio_data, sr):
        """éŸ³å£°ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°"""
        features = {}
        
        # åŸºæœ¬çš„ãªéŸ³é‡ç‰¹å¾´é‡ï¼ˆRMSï¼‰
        rms = librosa.feature.rms(y=audio_data)[0]
        times = librosa.times_like(rms, sr=sr)
        features['rms'] = rms
        features['times'] = times
        features['mean_volume'] = np.mean(rms)
        features['std_volume'] = np.std(rms)
        features['max_volume'] = np.max(rms)
        features['min_volume'] = np.min(rms)
        
        # ä¼šè©±éŸ³å£°ã‚’ä¸‰åˆ†å‰²ã—ãŸåˆ†æ
        third = len(rms) // 3
        features['start_volume'] = np.mean(rms[:third])  # æœ€åˆã®1/3
        features['middle_volume'] = np.mean(rms[third:2*third])  # ä¸­é–“ã®1/3
        features['end_volume'] = np.mean(rms[2*third:])  # æœ€å¾Œã®1/3
        
        # æ–‡æœ«éŸ³é‡ä½ä¸‹ç‡ã®è¨ˆç®—
        features['end_drop_rate'] = (features['middle_volume'] - features['end_volume']) / features['middle_volume'] if features['middle_volume'] > 0 else 0
        
        # ã‚ˆã‚Šè©³ç´°ãªæ–‡æœ«åˆ†æï¼ˆæœ€å¾Œã®20%éƒ¨åˆ†ï¼‰
        end_portion = max(1, int(len(rms) * 0.2))  # æœ€å¾Œã®20%
        features['last_20_percent_volume'] = np.mean(rms[-end_portion:])
        features['last_20_percent_drop_rate'] = (features['mean_volume'] - features['last_20_percent_volume']) / features['mean_volume'] if features['mean_volume'] > 0 else 0
        
        # MFCCç‰¹å¾´é‡ï¼ˆéŸ³å£°ã®éŸ³è‰²ç‰¹æ€§ï¼‰
        mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)
        for i in range(len(mfccs)):
            features[f'mfcc_{i+1}_mean'] = np.mean(mfccs[i])
            features[f'mfcc_{i+1}_std'] = np.std(mfccs[i])
        
        # ã‚¹ãƒšã‚¯ãƒˆãƒ«ç‰¹æ€§
        spectral_centroid = librosa.feature.spectral_centroid(y=audio_data, sr=sr)[0]
        features['spectral_centroid_mean'] = np.mean(spectral_centroid)
        
        # éŸ³å£°ã®ãƒšãƒ¼ã‚¹ï¼ˆã‚ªãƒ³ã‚»ãƒƒãƒˆæ¤œå‡ºã§éŸ³ç¯€ã‚’è¿‘ä¼¼ï¼‰
        onset_env = librosa.onset.onset_strength(y=audio_data, sr=sr)
        onsets = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)
        features['onset_count'] = len(onsets)
        features['speech_rate'] = len(onsets) / (len(audio_data) / sr) if len(audio_data) > 0 else 0
        
        return features

# éŸ³å£°åˆ†æã®ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã¨é–¢æ•°ã®å®šç¾©
class VoiceFeatureExtractor:
    """éŸ³å£°ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def extract_features(self, audio_data, sr):
        """éŸ³å£°ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°"""
        features = {}
        
        # åŸºæœ¬çš„ãªéŸ³é‡ç‰¹å¾´é‡ï¼ˆRMSï¼‰
        rms = librosa.feature.rms(y=audio_data)[0]
        times = librosa.times_like(rms, sr=sr)
        features['rms'] = rms
        features['times'] = times
        features['mean_volume'] = np.mean(rms)
        features['std_volume'] = np.std(rms)
        features['max_volume'] = np.max(rms)
        features['min_volume'] = np.min(rms)
        
        # ä¼šè©±éŸ³å£°ã‚’ä¸‰åˆ†å‰²ã—ãŸåˆ†æ
        third = len(rms) // 3
        features['start_volume'] = np.mean(rms[:third])  # æœ€åˆã®1/3
        features['middle_volume'] = np.mean(rms[third:2*third])  # ä¸­é–“ã®1/3
        features['end_volume'] = np.mean(rms[2*third:])  # æœ€å¾Œã®1/3
        
        # æ–‡æœ«éŸ³é‡ä½ä¸‹ç‡ã®è¨ˆç®—
        features['end_drop_rate'] = (features['middle_volume'] - features['end_volume']) / features['middle_volume'] if features['middle_volume'] > 0 else 0
        
        # ã‚ˆã‚Šè©³ç´°ãªæ–‡æœ«åˆ†æï¼ˆæœ€å¾Œã®20%éƒ¨åˆ†ï¼‰
        end_portion = max(1, int(len(rms) * 0.2))  # æœ€å¾Œã®20%
        features['last_20_percent_volume'] = np.mean(rms[-end_portion:])
        features['last_20_percent_drop_rate'] = (features['mean_volume'] - features['last_20_percent_volume']) / features['mean_volume'] if features['mean_volume'] > 0 else 0
        
        # MFCCç‰¹å¾´é‡ï¼ˆéŸ³å£°ã®éŸ³è‰²ç‰¹æ€§ï¼‰
        mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)
        for i in range(len(mfccs)):
            features[f'mfcc_{i+1}_mean'] = np.mean(mfccs[i])
            features[f'mfcc_{i+1}_std'] = np.std(mfccs[i])
        
        # ã‚¹ãƒšã‚¯ãƒˆãƒ«ç‰¹æ€§
        spectral_centroid = librosa.feature.spectral_centroid(y=audio_data, sr=sr)[0]
        features['spectral_centroid_mean'] = np.mean(spectral_centroid)
        
        # éŸ³å£°ã®ãƒšãƒ¼ã‚¹ï¼ˆã‚ªãƒ³ã‚»ãƒƒãƒˆæ¤œå‡ºã§éŸ³ç¯€ã‚’è¿‘ä¼¼ï¼‰
        onset_env = librosa.onset.onset_strength(y=audio_data, sr=sr)
        onsets = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)
        features['onset_count'] = len(onsets)
        features['speech_rate'] = len(onsets) / (len(audio_data) / sr) if len(audio_data) > 0 else 0
        
        return features

class VoiceQualityModel:
    """éŸ³å£°å“è³ªã‚’è©•ä¾¡ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«"""

    def __init__(self):
        """ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"""
        self.model = None
        self.scaler = StandardScaler()
        self.feature_extractor = VoiceFeatureExtractor()
        self.is_trained = False
        self.classes = None

    def prepare_features(self, features_dict):
        """ç‰¹å¾´è¾æ›¸ã‹ã‚‰æ©Ÿæ¢°å­¦ç¿’ç”¨ã®ç‰¹å¾´é‡é…åˆ—ã‚’ä½œæˆ"""
        # ä¸»è¦ãªç‰¹å¾´é‡ã®ã¿ã‚’æŠ½å‡º
        feature_keys = [
            'mean_volume', 'std_volume', 
            'start_volume', 'middle_volume', 'end_volume',
            'end_drop_rate', 'last_20_percent_volume', 'last_20_percent_drop_rate'
        ]

        # ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®ä½œæˆ
        features = []
        for key in feature_keys:
            if key in features_dict:
                features.append(features_dict[key])
            else:
                features.append(0)  # ç‰¹å¾´ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯0ã§åŸ‹ã‚ã‚‹
        
        # MFCCãªã©ã®è¿½åŠ ç‰¹å¾´ï¼ˆã‚ã‚Œã°è¿½åŠ ï¼‰
        if 'spectral_centroid_mean' in features_dict:
            features.append(features_dict['spectral_centroid_mean'])
        if 'speech_rate' in features_dict:
            features.append(features_dict['speech_rate'])

        return features

    def train(self, X, y):
        """ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹"""
        # ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–ï¼ˆç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰
        X_scaled = self.scaler.fit_transform(X)

        # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆåˆ†é¡å™¨ã‚’ä½œæˆ
        self.model = RandomForestClassifier(
            n_estimators=100,  # æ±ºå®šæœ¨ã®æ•°
            max_depth=10,      # æœ¨ã®æœ€å¤§æ·±ã•
            random_state=42    # å†ç¾æ€§ã®ãŸã‚ã®ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        )

        # ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
        self.model.fit(X_scaled, y)
        
        # ã‚¯ãƒ©ã‚¹ã®ãƒªã‚¹ãƒˆã‚’ä¿å­˜
        self.classes = self.model.classes_
        
        # è¨“ç·´æ¸ˆã¿ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
        self.is_trained = True

        return True
    
    def predict(self, features_dict):
        """éŸ³å£°å“è³ªã‚’äºˆæ¸¬ã™ã‚‹"""

        if not self.is_trained or self.model is None:
            return None, 0
    
        # ç‰¹å¾´é‡é…åˆ—ã‚’ä½œæˆ
        features = self.prepare_features(features_dict)
    
        # ç‰¹å¾´é‡ã‚’2æ¬¡å…ƒé…åˆ—ã«å¤‰æ›ï¼ˆsklearnè¦ä»¶ï¼‰
        features_2d = np.array([features])
    
        # ç‰¹å¾´é‡ã‚’æ¨™æº–åŒ–
        features_scaled = self.scaler.transform(features_2d)
    
        # äºˆæ¸¬å®Ÿè¡Œ
        prediction = self.model.predict(features_scaled)[0]
    
        # äºˆæ¸¬ç¢ºç‡ï¼ˆä¿¡é ¼åº¦ï¼‰ã‚’å–å¾—
        probabilities = self.model.predict_proba(features_scaled)[0]
        confidence = np.max(probabilities)
    
        return prediction, confidence

    def save_model(self, file_path):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹"""
        if not self.is_trained or self.model is None:
            # ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ãªã„å ´åˆã‚„å­˜åœ¨ã—ãªã„å ´åˆã¯ä¿å­˜ã—ãªã„
            return False
    
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¾æ›¸ã«ã¾ã¨ã‚ã‚‹
        model_info = {
            'model': self.model,
            'scaler': self.scaler,
            'is_trained': self.is_trained,
            'classes': self.classes
            }
    
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        joblib.dump(model_info, file_path)
        return True

    def load_model(self, file_path):
        """ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€
            model_info = joblib.load(file_path)
        
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å¾©å…ƒ
            self.model = model_info['model']
            self.scaler = model_info['scaler']
            self.is_trained = model_info['is_trained']
            self.classes = model_info['classes']
        
            return True
        except Exception as e:
            print(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
 




