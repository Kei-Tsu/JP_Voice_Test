import numpy as np
import librosa
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import joblib
import logging
import pandas as pd
import streamlit as st

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = logging.getLogger(__name__)
# å¿…è¦ãªã‚¯ãƒ©ã‚¹ã¨é–¢æ•°ã‚’å®Ÿè£…
class VoiceQualityModel:
    """éŸ³å£°å“è³ªã‚’è©•ä¾¡ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«

    ã“ã®ã‚¯ãƒ©ã‚¹ã¯éŸ³å£°ã®ç‰¹å¾´é‡ã‹ã‚‰ä¼šè©±éŸ³å£°ã®å“è³ªã‚’åˆ¤æ–­ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
    ä¸»ã«ã€Œè‰¯å¥½ã€ã€Œæ–‡æœ«ãŒå¼±ã„ã€ã€Œå°å£°ã™ãã‚‹ã€ã®3ã¤ã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã—ã¾ã™ã€‚
    """

    def __init__(self):
        """ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"""
        self.model = None # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
        self.scaler = StandardScaler()
        self.is_trained = False
        self.classes = None

        # ç‰¹å¾´é‡ã®åå‰ï¼ˆæ—¥æœ¬èªã§è¡¨ç¤ºç”¨ï¼‰
        self.feature_names = [
            'å¹³å‡éŸ³é‡', 'éŸ³é‡å¤‰å‹•', 'æ–‡é ­éŸ³é‡', 'æ–‡ä¸­éŸ³é‡', 'æ–‡æœ«éŸ³é‡',
            'éŸ³é‡ä½ä¸‹ç‡', 'æœ€å¾Œ20%éŸ³é‡', 'æœ€å¾Œ20%ä½ä¸‹ç‡', 'ã‚¹ãƒšã‚¯ãƒˆãƒ«é‡å¿ƒ', 'è©±ã®é€Ÿåº¦'
        ]
        # ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è¨˜éŒ²ç”¨
        self.training_accuracy = 0
        self.test_accuracy = 0

    def prepare_features(self, features_dict):
        """ç‰¹å¾´è¾æ›¸ã‹ã‚‰æ©Ÿæ¢°å­¦ç¿’ç”¨ã®ç‰¹å¾´é‡é…åˆ—ã‚’ä½œæˆ"""
        # ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã®ã‚­ãƒ¼ï¼švoice_analysis.py ã‹ã‚‰ã®è¾æ›¸
        feature_keys = [
            'mean_volume', 
            'std_volume', 
            'start_volume', 
            'middle_volume', 
            'end_volume',
            'end_drop_rate',
            'last_20_percent_volume',
            'last_20_percent_drop_rate',
            'spectral_centroid_mean',
            'speech_rate'
        ]

        # ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®ä½œæˆ
        features = []
        for key in feature_keys:
            if key in features_dict:
                value = features_dict[key]
                # Nanã‚„ç„¡é™å¤§ä¾¡ã®å‡¦ç†ï¼ˆ0ã«ç½®ãæ›ãˆï¼‰
                if np.isnan(value) or np.isinf(value):
                    features.append(0.0)
                else:
                    features.append(float(value))   
            else:
                features.append(0.0) # ç‰¹å¾´å€¤ãŒå­˜åœ¨ã—ãªã„å ´åˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤0ã§åŸ‹ã‚ã‚‹ãŸã‚è¿½åŠ 
    
            return features
      
    def train(self, X, y):
        """ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹
        å¼•æ•°:
            X (np.ndarray): ç‰¹å¾´é‡
            y (np.ndarray): ãƒ©ãƒ™ãƒ«
        Returns:
            bool: è¨“ç·´ãŒæˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            st.write(f"**è¨“ç·´ãƒ‡ãƒ¼ã‚¿è©³ç´°**")
            st.write(f"- ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X)}")

            # å„ã‚¯ãƒ©ã‚¹ã®æ•°ã‚’ç¢ºèª
            unique_labels, counts = np.unique(y, return_counts=True)
            st.write("- ã‚¯ãƒ©ã‚¹åˆ¥ãƒ‡ãƒ¼ã‚¿æ•°:")
            for label, count in zip(unique_labels, counts):
                st.write(f"  - {label}: {count}å€‹")
        
            # ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
            if len(X) == 0 or len(y) == 0:
                st.error("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                logger.error("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                return False
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
            st.write(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²: {len(X_train)}è¨“ç·´, {len(X_test)}ãƒ†ã‚¹ãƒˆ")

            # NaNã‚„ç„¡é™å¤§å€¤ã®å‡¦ç†
            X_train_clean = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)
            X_test_clean = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)
            st.write(f"ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: è¨“ç·´{X_train_clean.shape}, ãƒ†ã‚¹ãƒˆ{X_test_clean.shape}")

            # ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–(å¹³å‡0, æ¨™æº–åå·®1)
            X_train_scaled = self.scaler.fit_transform(X_train_clean)
            X_test_scaled = self.scaler.transform(X_test_clean)
            st.write(f"ç‰¹å¾´é‡ã®æ¨™æº–åŒ–ãŒå®Œäº†")

            # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆåˆ†é¡å™¨ã‚’ä½œæˆ
            self.model = RandomForestClassifier(
                n_estimators=200,  # æ±ºå®šæœ¨ã®æ•°
                max_depth=15,      # æœ¨ã®æœ€å¤§æ·±ã•
                min_samples_split=5,  # åˆ†å‰²ã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
                min_samples_leaf=2,   # è‘‰ã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
                random_state=42,   # å†ç¾æ€§ã®ãŸã‚ã®ä¹±æ•°ã‚·ãƒ¼ãƒ‰
                n_jobs=-1,         # ä¸¦åˆ—å‡¦ç†ã‚’ä½¿ç”¨
                class_weight='balanced'  # ã‚¯ãƒ©ã‚¹ã®ä¸å‡è¡¡ã‚’è€ƒæ…®
            )
            st.write(f"ğŸ¤– ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä½œæˆ: 200æœ¬ã®æ±ºå®šæœ¨ã‚’æº–å‚™")

            # ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
            st.write("**å­¦ç¿’ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...**")
            self.model.fit(X_train_scaled, y_train)
            st.write("**å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸï¼**")

            # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®ç²¾åº¦ã‚’ç¢ºèª
            train_accuracy = self.model.score(X_train_scaled, y_train)
            test_accuracy = self.model.score(X_test_scaled, y_test)

            st.success(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®ç²¾åº¦: {train_accuracy:.1%}")
            st.success(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®ç²¾åº¦: {test_accuracy:.1%}")

            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®è©³ç´°ãªè©•ä¾¡
            y_pred = self.model.predict(X_test_scaled)

            # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º
            st.write("**è©³ç´°ãªè©•ä¾¡çµæœ:**")
            report = classification_report(y_test, y_pred, target_names=self.classes, output_dict=True)

            # å„ã‚¯ãƒ©ã‚¹ã®æ€§èƒ½ã‚’è¡¨ç¤º
            for class_name in self.model.classea_:
                if class_name in report:
                    precision = report[class_name]['precision']
                    recall = report[class_name]['recall']
                    f1_score = report[class_name]['f1-score']
                    st.write(f"- **{class_name}**: é©åˆç‡={precision:.2f}, å†ç¾ç‡={recall:.2f}, F1={f1_score:.2f}")

            # å…¨ä½“ã®æ€§èƒ½ã‚’è¡¨ç¤º(ãƒã‚¯ãƒ­å¹³å‡)
            macro_avg = report['macro avg']
            weighted_avg = report['weighted avg']

            st.write("**å…¨ä½“ã®æ€§èƒ½ã‚’è¡¨ç¤º(ãƒã‚¯ãƒ­å¹³å‡)**:")
            st.write(f"- **ãƒã‚¯ãƒ­å¹³å‡**: ç²¾åº¦={macro_avg['precision']:.2f}, å†ç¾ç‡={macro_avg['recall']:.2f}, F1ã‚¹ã‚³ã‚¢={macro_avg['f1-score']:.2f}")
            st.write(f"- **åŠ é‡å¹³å‡**: ç²¾åº¦={weighted_avg['precision']:.2f}, å†ç¾ç‡={weighted_avg['recall']:.2f}, F1ã‚¹ã‚³ã‚¢={weighted_avg['f1-score']:.2f}")

            # æ€§èƒ½ã®è§£é‡ˆ
            if macro_avg['f1-score'] >= 0.8:
                st.success("ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯è‰¯å¥½ã§ã™ï¼å„ã‚¯ãƒ©ã‚¹ã‚’ãƒãƒ©ãƒ³ã‚¹ã‚ˆãäºˆæ¸¬ã§ãã¦ã„ã¾ã™ã€‚")
            elif macro_avg['f1-score'] >= 0.7:
                st.warning("ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯æ™®é€šã§ã™ã€‚å®Ÿç”¨ãªãƒ¬ãƒ™ãƒ«ã«é”ã—ã¦ã„ã¾ã™ã€‚")
            elif macro_avg['f1-score'] >= 0.6:
                st.warning("ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯æ™®é€šã§ã™ã€‚æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚")
            else:
                st.error("ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯ä½ã„ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã®è³ªã‚„é‡ã‚’è¦‹ç›´ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")               
                            
            # ã‚¯ãƒ©ã‚¹ã®ãƒªã‚¹ãƒˆã‚’ä¿å­˜
            self.classes = self.model.classes_
            st.write(f"å­¦ç¿’ã—ãŸã‚¯ãƒ©ã‚¹: {list(self.classes)}")

           # ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å–å¾—
            importances = self.model.feature_importances_
            st.write(f"ç‰¹å¾´é‡ã®é‡è¦åº¦: {importances}")
            importance_data = []
            for name, importance in zip(self.feature_names[:len(importances)], importances):
                importance_data.append([name, f"{importance:.3f}"])

            # è¡¨å½¢å¼ã§è¡¨ç¤º
            importance_df = pd.DataFrame(importance_data, columns=['ç‰¹å¾´é‡', 'é‡è¦åº¦'])
            importance_df = importance_df.sort_values(by='é‡è¦åº¦', ascending=False)
            st.dataframe(importance_df)

            # æœ€ã‚‚é‡è¦ãªç‰¹å¾´é‡ã‚’å¼·èª¿
            top_features = importance_df.iloc[:3]['ç‰¹å¾´é‡']
            st.info(f"**æœ€é‡è¦ç‰¹å¾´é‡**: {', '.join(top_features)} - ã“ã®ç‰¹å¾´ãŒAIã®åˆ¤æ–­ã«æœ€ã‚‚å½±éŸ¿ã—ã¦ã„ã¾ã™")
            
            # è¨“ç·´æ¸ˆã¿ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
            self.is_trained = True

            logger.info(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†: {len(X)}ã‚µãƒ³ãƒ—ãƒ«, {len(self.classes)}ã‚¯ãƒ©ã‚¹")
            return True
        
        except Exception as e:
            st.error(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            return False
        
    def predict(self, features_dict, realtime=False):
        """éŸ³å£°å“è³ªã‚’äºˆæ¸¬ã™ã‚‹
        å¼•æ•°:
            features_dict (dict): voice_analysis.py ã‹ã‚‰ã®ç‰¹å¾´é‡ã®è¾æ›¸
        
        æˆ»ã‚Šå€¤
            tuple: äºˆæ¸¬çµæœã¨ä¿¡é ¼åº¦
        """
        try:
            if not self.is_trained or self.model is None:
                logger.warning("ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return None, 0
        
            # ç‰¹å¾´é‡é…åˆ—ã‚’ä½œæˆ
            features = self.prepare_features_realtime(features_dict)

            # ç‰¹å¾´é‡ã‚’2æ¬¡å…ƒé…åˆ—ã«å¤‰æ›ï¼ˆsklearnè¦ä»¶ï¼‰
            features_2d = np.array([features])

            # NaNã‚„ç„¡é™å¤§å€¤ã®å‡¦ç†
            features_2d = np.nan_to_num(features_2d, nan=0.0, posinf=0.0, neginf=0.0)

            # ç‰¹å¾´é‡ã‚’æ¨™æº–åŒ–
            features_scaled = self.scaler.transform(features_2d)
        
            # äºˆæ¸¬å®Ÿè¡Œ
            prediction = self.model.predict(features_scaled)[0]
        
            # äºˆæ¸¬ç¢ºç‡ï¼ˆä¿¡é ¼åº¦ï¼‰ã‚’å–å¾—
            probabilities = self.model.predict_proba(features_scaled)[0]
            confidence = np.max(probabilities)
        
            return prediction, confidence
 
        except Exception as e:
            logger.error(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            return None, 0
        
    def get_feature_importance(self):
        """ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å–å¾—
        æˆ»ã‚Šå€¤:
            dict: ç‰¹å¾´é‡ã®é‡è¦åº¦
        """
        try:
            if not self.is_trained or self.model is None:
                return None

            importances = self.model.feature_importances_
            feature_importance = dict(zip(self.feature_names, importances))
            return feature_importance
        
        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡é‡è¦åº¦å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def get_model_performance(self):
        """ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å–å¾—
        æˆ»ã‚Šå€¤:
            dict: ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æƒ…å ±
        """
        if self.is_trained:
                return {
                    'training_accuracy': self.training_accuracy,
                    'test_accuracy': self.test_accuracy,
                    'feature_count': len(self.feature_names),
                    'class_count': len(self.classes) if self.classes is not None else 0
                }
        return None

    def save_model(self, file_path):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹
        å¼•æ•°:
            file_path (str): ä¿å­˜å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        æˆ»ã‚Šå€¤:
            bool: ä¿å­˜æˆåŠŸãªã‚‰Trueã€å¤±æ•—ãªã‚‰False   
        """
        try:
            if not self.is_trained or self.model is None:
                logger.warning("ä¿å­˜ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
                return False
            
            model_info = {
                'model': self.model,
                'scaler': self.scaler,
                'is_trained': self.is_trained,
                'classes': self.classes,
                'feature_names': self.feature_names,
                'training_accuracy': self.training_accuracy,
                'test_accuracy': self.test_accuracy
                }

            joblib.dump(model_info, file_path)
            logger.info(f"ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {file_path}")
            return True
        
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False
        
    def load_model(self,file_path):
        """ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        å¼•æ•°:
        file_path (str): èª­ã¿è¾¼ã¿å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        try:
            # ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€
            model_info = joblib.load(file_path)

            self.model = model_info['model']
            self.scaler = model_info['scaler']
            self.is_trained = model_info['is_trained']
            self.classes = model_info['classes']
            self.feature_names = model_info.get('feature_names', self.feature_names)
            self.training_accuracy = model_info.get('training_accuracy', 0)

            logger.info(f"ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False

# ã‚¯ãƒ©ã‚¹å¤–ã®ç‹¬ç«‹ã—ãŸé–¢æ•°ã¨ã—ã¦å®šç¾©
def generate_training_data():
    """æ©Ÿæ¢°å­¦ç¿’ç”¨ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
    å¼•æ•°:
        ãªã—
    æˆ»ã‚Šå€¤:
        tuple: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã¨ãƒ©ãƒ™ãƒ«
    """
    try:
        x = []  # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
        y = []  # ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿

        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        for i in range(80):
            features = [
                np.random.uniform(0.08, 0.25),   # mean_volume
                np.random.uniform(0.015, 0.04),   # std_volume
                np.random.uniform(0.08, 0.25),     # start_volume
                np.random.uniform(0.08, 0.25),     # middle_volume
                np.random.uniform(0.07, 0.22),   # end_volumeï¼ˆãã“ã¾ã§ä½ä¸‹ã—ãªã„ï¼‰
                np.random.uniform(0.03, 0.12),   # end_drop_rateï¼ˆå°ã•ã‚ï¼‰
                np.random.uniform(0.07, 0.22),   # last_20_percent_volume
                np.random.uniform(0.03, 0.12),   # last_20_percent_drop_rate
                np.random.uniform(1200, 2200),   # spectral_centroid_mean
                np.random.uniform(2.5, 4.5),     # speech_rate
            ]

            x.append(features)
            y.append("è‰¯å¥½")  

        # ã€Œæ–‡æœ«ãŒå¼±ã„ã€éŸ³å£°ã®ãƒ‡ãƒ¼ã‚¿
        for i in range(80):
            features = [
                np.random.uniform(0.08, 0.25),   # mean_volume
                np.random.uniform(0.015, 0.04),   # std_volume
                np.random.uniform(0.08, 0.25),     # start_volume
                np.random.uniform(0.08, 0.25),     # middle_volume
                np.random.uniform(0.02, 0.08),      # end_volumeï¼ˆæ˜ã‚‰ã‹ã«ä½ã„ï¼‰
                np.random.uniform(0.25, 0.6),      # end_drop_rateï¼ˆå¤§ãã„ï¼‰
                np.random.uniform(0.02, 0.08),      # last_20_percent_volume
                np.random.uniform(0.25, 0.6),      # last_20_percent_drop_rate
                np.random.uniform(1000, 2000),     # spectral_centroid_mean
                np.random.uniform(2, 4),           # speech_rate
            ]
            x.append(features)
            y.append("æ–‡æœ«ãŒå¼±ã„")

        # ã€Œå°å£°ã™ãã‚‹ã€éŸ³å£°ã®ãƒ‡ãƒ¼ã‚¿
        for _ in range(50): 
            features = [
                np.random.uniform(0.005, 0.04),  # mean_volumeï¼ˆå…¨ä½“çš„ã«å°ã•ã„ï¼‰
                np.random.uniform(0.005, 0.015), # std_volume
                np.random.uniform(0.005, 0.04),  # start_volume
                np.random.uniform(0.005, 0.04),  # middle_volume
                np.random.uniform(0.003, 0.025), # end_volume
                np.random.uniform(0.1, 0.35),    # end_drop_rate
                np.random.uniform(0.003, 0.025), # last_20_percent_volume
                np.random.uniform(0.1, 0.35),    # last_20_percent_drop_rate
                np.random.uniform(800, 1400),    # spectral_centroid_meanï¼ˆä½ã‚ï¼‰
                np.random.uniform(1.5, 3),       # speech_rateï¼ˆé…ã‚ï¼‰        
            ]
            x.append(features)
            y.append("å°å£°ã™ãã‚‹")

        logger.info(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {len(x)}ã‚µãƒ³ãƒ—ãƒ«")
        return np.array(x), np.array(y)

    except Exception as e:
        logger.error(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return np.array([]), np.array([])

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜ãƒ»èª­ã¿è¾¼ã¿æ©Ÿèƒ½
def save_training_data(X, y, file_path):
    """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    å¼•æ•°:
        X (np.ndarray): ç‰¹å¾´é‡
        y (np.ndarray): ãƒ©ãƒ™ãƒ«
        file_path (str): ä¿å­˜å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    æˆ»ã‚Šå€¤:
        bool: ä¿å­˜æˆåŠŸãªã‚‰Trueã€å¤±æ•—ãªã‚‰False
    """
    try:
        np.savez(file_path, X=X, y=y)
        logger.info(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {file_path}")
        return True
    except Exception as e:
        logger.error(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def load_training_data(file_path):
    """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
    å¼•æ•°:
        file_path (str): èª­ã¿è¾¼ã¿å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    æˆ»ã‚Šå€¤:
        tuple: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã¨ãƒ©ãƒ™ãƒ«
        """
    try:
        data = np.load(file_path)
        X = data['X']
        y = data['y']
        logger.info(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {file_path}")
        return X, y
    except Exception as e:
        logger.error(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return np.array([]), np.array([])
    
# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°å“è³ªè©•ä¾¡ç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
def quick_quality_assessment(features_dict):
    """è»½é‡ãªéŸ³å£°å“è³ªè©•ä¾¡ï¼ˆæ©Ÿæ¢°å­¦ç¿’ãªã—ï¼‰
    æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ç°¡æ˜“è©•ä¾¡ã€‚
    ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§ä¼šè©±ã®å“è³ªã‚’è©•ä¾¡ã—ã¾ã™ã€‚

    å¼•æ•°:
        features_dict (dict): éŸ³å£°ç‰¹å¾´é‡ã®è¾æ›¸
    æˆ»ã‚Šå€¤:
        tuple: è©•ä¾¡çµæœã¨ä¿¡é ¼åº¦
        """
    try:
        drop_rate = features_dict.get('end_drop_rate', 0)
        mean_volume = features_dict.get('mean_volume', 0)
        
        # ã‚ˆã‚Šè©³ç´°ãªãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è©•ä¾¡
        if drop_rate < 0.1 and mean_volume > 0.05:
            return "è‰¯å¥½", 0.95
        elif drop_rate < 0.15 and mean_volume > 0.03:
            return "è‰¯å¥½", 0.85
        elif drop_rate < 0.25 and mean_volume > 0.02:
            return "æ™®é€š", 0.7
        elif drop_rate < 0.4:
            return "æ–‡æœ«ãŒå¼±ã„", 0.6
        elif mean_volume < 0.02:
            return "å°å£°ã™ãã‚‹", 0.5
        else:
            return "ä¼šè©±ã‚’ã‚‚ã†å°‘ã—æ„è­˜ã—ã¦ã¿ã¾ã—ã‚‡ã†", 0.4
            
    except Exception as e:
        logger.error(f"å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        return "è©•ä¾¡ä¸å¯", 0.0
    
def create_dataset_from_files(file_paths):
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹é–¢æ•°ï¼ˆå°†æ¥ã®æ‹¡å¼µç”¨ï¼‰"""
    """
    ã“ã®é–¢æ•°ã¯å°†æ¥ã€å®Ÿéš›ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã¦
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã¾ã™ã€‚
    ç¾åœ¨ã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã¨ã—ã¦ç©ºã®å®Ÿè£…ã«ãªã£ã¦ã„ã¾ã™ã€‚
    """
    try:
        # å°†æ¥ã®å®Ÿè£…:
        # 1. å„éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º
        # 2. ãƒ©ãƒ™ãƒ«ã‚’è¨­å®šï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‚„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
        # 3. ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã‚’ã¾ã¨ã‚ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
        
        logger.info("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã¯ä»Šå¾Œå®Ÿè£…äºˆå®šã§ã™")
        return np.array([]), np.array([])
        
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        return np.array([]), np.array([])

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°å“è³ªè©•ä¾¡ç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
def quick_quality_assessment(features_dict):
    """è»½é‡ãªéŸ³å£°å“è³ªè©•ä¾¡ï¼ˆæ©Ÿæ¢°å­¦ç¿’ãªã—ï¼‰"""
    try:
        drop_rate = features_dict.get('end_drop_rate', 0)
        mean_volume = features_dict.get('mean_volume', 0)
        
        # ç°¡å˜ãªãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è©•ä¾¡
        if drop_rate < 0.15 and mean_volume > 0.05:
            return "è‰¯å¥½", 0.9
        elif drop_rate < 0.3:
            return "æ™®é€š", 0.7
        else:
            return "è¦æ”¹å–„", 0.5
            
    except Exception as e:
        logger.error(f"å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        return "è©•ä¾¡ä¸å¯", 0.0