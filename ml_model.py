import numpy as np
import streamlit as st
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import logging
import pandas as pd
import sys
import traceback


# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class VoiceQualityModel:
    """éŸ³å£°å“è³ªã‚’è©•ä¾¡ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self):
        """ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"""
        self.model = None # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
        self.scaler = StandardScaler()
        self.is_trained = False
        self.classes = None

        # ç‰¹å¾´é‡ã®åå‰ï¼ˆæ—¥æœ¬èªã§è¡¨ç¤ºç”¨ï¼‰
        self.feature_names = [
            'Mean Volume(å¹³å‡éŸ³é‡)', 'Volume Variation(éŸ³é‡å¤‰å‹•)', 'Start Volume(æ–‡é ­éŸ³é‡)', 'Middle Volume(æ–‡ä¸­éŸ³é‡)', 'End Volume(æ–‡æœ«éŸ³é‡)',
            'Volume Drop Rate(éŸ³é‡ä½ä¸‹ç‡)', 'Last 20% Volume(æœ€å¾Œ20%éŸ³é‡)', 'Last 20% Drop Rate(æœ€å¾Œ20%ä½ä¸‹ç‡)', 'Spectral Centroid(ã‚¹ãƒšã‚¯ãƒˆãƒ«é‡å¿ƒ)', 'Speech Rate(è©±ã®é€Ÿåº¦)'
        ]
        
        # ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è¨˜éŒ²ç”¨
        self.training_accuracy = 0
        self.test_accuracy = 0

    def prepare_features(self, features_dict):
        """ç‰¹å¾´è¾æ›¸ã‹ã‚‰æ©Ÿæ¢°å­¦ç¿’ç”¨ã®ç‰¹å¾´é‡é…åˆ—ã‚’ä½œæˆ"""
        # ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã®ã‚­ãƒ¼ã‹ã‚‰ã®è¾æ›¸
        feature_keys = [
            'mean_volume', 
            'std_volume', 
            'start_volume', 
            'middle_volume', 
            'end_volume',
            'end_drop_rate',
            'last_20_percent_volume',
            'last_20_percent_drop_rate',
            'spectral_centroid_mean',
            'speech_rate'
        ]
        
        features = []
        for key in feature_keys:
            if key in features_dict:
                value = features_dict[key]
                # Nanã‚„ç„¡é™å¤§ä¾¡ã®å‡¦ç†ï¼ˆ0ã«ç½®ãæ›ãˆï¼‰
                if np.isnan(value) or np.isinf(value):
                    features.append(0.0)
                else:
                    features.append(float(value))   
            else:
                features.append(0.0) # ç‰¹å¾´å€¤ãŒå­˜åœ¨ã—ãªã„å ´åˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤0ã§åŸ‹ã‚ã‚‹ãŸã‚è¿½åŠ 
    
        return features
      
    def train(self, X, y):
        """ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹"""
        try:
            #ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ï¼ˆæœ€åˆã«å®Ÿè¡Œï¼‰
            if len(X) == 0 or len(y) ==0:
                st.error("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                logger.error(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                return False
            
            st.write("**AIè¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™**")
            st.write(f"ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X)}")           

            # å„ã‚¯ãƒ©ã‚¹ã®æ•°ã‚’ç¢ºèª
            unique_labels, counts = np.unique(y, return_counts=True)
            st.write("ã‚¯ãƒ©ã‚¹åˆ¥ãƒ‡ãƒ¼ã‚¿æ•°:")
            for label, count in zip(unique_labels, counts):
                st.write(f"  - {label}: {count}å€‹")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²
            st.write("ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²ä¸­...")
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

            st.write(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†: è¨“ç·´{len(X_train)}å€‹, ãƒ†ã‚¹ãƒˆ{len(X_test)}å€‹")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° (NaNã‚„ç„¡é™å¤§å€¤ã®å‡¦ç†)
            st.write("ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...")
            X_train_clean = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)
            X_test_clean = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)
            st.write(f"ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")

            # ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–(å¹³å‡0, æ¨™æº–åå·®1)
            st.write("ç‰¹å¾´é‡ã®æ¨™æº–åŒ–ä¸­...")
            X_train_scaled = self.scaler.fit_transform(X_train_clean)
            X_test_scaled = self.scaler.transform(X_test_clean)
            st.write(f"ç‰¹å¾´é‡ã®æ¨™æº–åŒ–ãŒå®Œäº†")

            # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆåˆ†é¡å™¨ã‚’ä½œæˆ
            st.write("AIãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
            self.model = RandomForestClassifier(
                n_estimators=100,  # æ±ºå®šæœ¨ã®æ•°
                max_depth=10,      # æœ¨ã®æœ€å¤§æ·±ã•
                min_samples_split=5,  # åˆ†å‰²ã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
                min_samples_leaf=2,   # è‘‰ã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
                random_state=42,   # å†ç¾æ€§ã®ãŸã‚ã®ä¹±æ•°ã‚·ãƒ¼ãƒ‰
                n_jobs=-1,         # ä¸¦åˆ—å‡¦ç†ã‚’ä½¿ç”¨
                class_weight='balanced'  # ã‚¯ãƒ©ã‚¹ã®ä¸å‡è¡¡ã‚’è€ƒæ…®
            )
            st.write(f"ğŸ¤– ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä½œæˆ: 100æœ¬ã®æ±ºå®šæœ¨ã‚’æº–å‚™")

            # ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
            st.write("**å­¦ç¿’ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...**")
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’è¡¨ç¤º
            progress_bar = st.progress(0)
            progress_bar.progress(50)            
            
            self.model.fit(X_train_scaled, y_train)
            st.write("**AIå­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸï¼**")

            # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®ç²¾åº¦ã‚’ç¢ºèª
            st.write("ç²¾åº¦è©•ä¾¡ä¸­...")
            train_accuracy = self.model.score(X_train_scaled, y_train)
            test_accuracy = self.model.score(X_test_scaled, y_test)

            st.success(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®ç²¾åº¦: {train_accuracy:.1%}")
            st.success(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®ç²¾åº¦: {test_accuracy:.1%}")

            # ã‚¯ãƒ©ã‚¹æƒ…å ±ã‚’ä¿å­˜
            self.classes = self.model.classes_
            self.is_trained = True
            self.training_accuracy = train_accuracy
            self.test_accuracy = test_accuracy
            
            st.write(f"å­¦ç¿’ã—ãŸã‚¯ãƒ©ã‚¹: {list(self.classes)}")

            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®è©³ç´°ãªè©•ä¾¡
            st.write("è©³ç´°ãªè©•ä¾¡çµæœã‚’è¨ˆç®—ä¸­...")
            y_pred = self.model.predict(X_test_scaled)
            report = classification_report(y_test, y_pred, target_names=unique_labels, output_dict=True)

            # å„ã‚¯ãƒ©ã‚¹ã®æ€§èƒ½ã‚’è¡¨ç¤º
            st.write("**å„ã‚¯ãƒ©ã‚¹ã®æ€§èƒ½:**")
            for class_name in unique_labels:
                if class_name in report:
                    precision = report[class_name]['precision']
                    recall = report[class_name]['recall']
                    f1_score = report[class_name]['f1-score']
                    st.write(f"- **{class_name}**: é©åˆç‡={precision:.2f}, å†ç¾ç‡={recall:.2f}, F1={f1_score:.2f}")

            # å…¨ä½“ã®æ€§èƒ½ã‚’è¡¨ç¤º(ãƒã‚¯ãƒ­å¹³å‡)
            macro_avg = report['macro avg']
            st.write(f"**å…¨ä½“F1ã‚¹ã‚³ã‚¢**: {macro_avg['f1-score']:.2f}")          
            
            # æ€§èƒ½ã®è§£é‡ˆ
            if macro_avg['f1-score'] >= 0.8:
                st.success("ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯è‰¯å¥½ã§ã™ï¼å„ã‚¯ãƒ©ã‚¹ã‚’ãƒãƒ©ãƒ³ã‚¹ã‚ˆãäºˆæ¸¬ã§ãã¦ã„ã¾ã™ã€‚")
            elif macro_avg['f1-score'] >= 0.7:
                st.info("ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯æ™®é€šã§ã™ã€‚å®Ÿç”¨ãªãƒ¬ãƒ™ãƒ«ã«é”ã—ã¦ã„ã¾ã™ã€‚")
            elif macro_avg['f1-score'] >= 0.6:
                st.warning("ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯æ™®é€šã§ã™ã€‚æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚")
            else:
                st.error("ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯ä½ã„ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã®è³ªã‚„é‡ã‚’è¦‹ç›´ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")               
                            
            # ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å–å¾—
            st.write("ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’åˆ†æä¸­...")
            importances = self.model.feature_importances_
            importance_df = pd.DataFrame({
                'ç‰¹å¾´é‡': self.feature_names[:len(importances)],
                'é‡è¦åº¦': [f"{imp:.3f}" for imp in importances]
            }).sort_values('é‡è¦åº¦', ascending=False)

            st.write("**ç‰¹å¾´é‡ã®é‡è¦åº¦:**")
            st.dataframe(importance_df)

            # æœ€é‡è¦ç‰¹å¾´é‡ã‚’å¼·èª¿
            top_features = importance_df.iloc[:3]['ç‰¹å¾´é‡'].tolist()
            st.info(f"**æœ€é‡è¦ç‰¹å¾´é‡**: {', '.join(top_features)} - ã“ã®ç‰¹å¾´ãŒAIã®åˆ¤æ–­ã«æœ€ã‚‚å½±éŸ¿ã—ã¦ã„ã¾ã™")

            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
            progress_bar.empty()
            
            st.success("**AIè¨“ç·´ãŒå®Œå…¨ã«å®Œäº†ã—ã¾ã—ãŸï¼** ã“ã‚Œã§ã€Œç·´ç¿’ã‚’å§‹ã‚ã‚‹ã€ãƒšãƒ¼ã‚¸ã§é«˜ç²¾åº¦ãªéŸ³å£°åˆ†æãŒåˆ©ç”¨ã§ãã¾ã™ã€‚")
            
            logger.info(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†: {len(X)}ã‚µãƒ³ãƒ—ãƒ«, {len(self.classes)}ã‚¯ãƒ©ã‚¹")            
            return True         
                  
        except Exception as e:
            st.error(f"AIè¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"è¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            return False            
                       
    def predict(self, features_dict):
        """éŸ³å£°å“è³ªã‚’äºˆæ¸¬ã™ã‚‹"""
        try:
            if not self.is_trained or self.model is None:
                logger.warning("ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return None, 0
        
            # ç‰¹å¾´é‡é…åˆ—ã‚’ä½œæˆ
            features = self.prepare_features(features_dict)

            # ç‰¹å¾´é‡ã‚’2æ¬¡å…ƒé…åˆ—ã«å¤‰æ›ï¼ˆsklearnè¦ä»¶ï¼‰
            features_2d = np.array([features])

            # NaNã‚„ç„¡é™å¤§å€¤ã®å‡¦ç†
            features_2d = np.nan_to_num(features_2d, nan=0.0, posinf=0.0, neginf=0.0)

            # ç‰¹å¾´é‡ã‚’æ¨™æº–åŒ–
            features_scaled = self.scaler.transform(features_2d)
        
            # äºˆæ¸¬å®Ÿè¡Œ
            prediction = self.model.predict(features_scaled)[0]
        
            # äºˆæ¸¬ç¢ºç‡ï¼ˆä¿¡é ¼åº¦ï¼‰ã‚’å–å¾—
            probabilities = self.model.predict_proba(features_scaled)[0]
            confidence = np.max(probabilities)
        
            return prediction, confidence
 
        except Exception as e:
            logger.error(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            return None, 0
        
    def get_feature_importance(self):
        """ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å–å¾—"""
        try:
            if not self.is_trained or self.model is None:
                return None

            importances = self.model.feature_importances_
            feature_importance = dict(zip(self.feature_names, importances))
            return feature_importance
        
        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡é‡è¦åº¦å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def get_model_performance(self):
        """ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å–å¾—"""
        if self.is_trained:
                return {
                    'training_accuracy': self.training_accuracy,
                    'test_accuracy': self.test_accuracy,
                    'feature_count': len(self.feature_names),
                    'class_count': len(self.classes) if self.classes is not None else 0
                }
        return None

# ã‚¯ãƒ©ã‚¹å¤–ã®ç‹¬ç«‹ã—ãŸé–¢æ•°ã¨ã—ã¦å®šç¾©
def generate_training_data():
    """æ©Ÿæ¢°å­¦ç¿’ç”¨ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°"""
    try:
        x = []  # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
        y = []  # ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿

        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        # ã€Œè‰¯å¥½ã€ãªéŸ³å£°ãƒ‡ãƒ¼ã‚¿ï¼ˆ80å€‹ï¼‰
        for i in range(70):
            features = [
                np.random.uniform(0.08, 0.25),   # mean_volume
                np.random.uniform(0.015, 0.04),   # std_volume
                np.random.uniform(0.08, 0.25),     # start_volume
                np.random.uniform(0.08, 0.25),     # middle_volume
                np.random.uniform(0.07, 0.22),   # end_volumeï¼ˆãã“ã¾ã§ä½ä¸‹ã—ãªã„ï¼‰
                np.random.uniform(0.03, 0.12),   # end_drop_rateï¼ˆå°ã•ã‚ï¼‰
                np.random.uniform(0.07, 0.22),   # last_20_percent_volume
                np.random.uniform(0.03, 0.12),   # last_20_percent_drop_rate
                np.random.uniform(1200, 2200),   # spectral_centroid_mean
                np.random.uniform(2.5, 4.5),     # speech_rate
            ]

            # ãƒã‚¤ã‚ºã‚’è¿½åŠ ï¼ˆç¾å®Ÿçš„ãªãƒãƒ©ãƒ„ã‚­ã‚’å†ç¾ï¼‰
            noise = np.random.normal(0, 0.01, len(features))
            features = np.array(features) + noise


            x.append(features)
            y.append("è‰¯å¥½")  

        # ã€Œæ–‡æœ«ãŒå¼±ã„ã€éŸ³å£°ã®ãƒ‡ãƒ¼ã‚¿
        for i in range(70):
            features = [
                np.random.uniform(0.08, 0.25),   # mean_volume
                np.random.uniform(0.015, 0.04),   # std_volume
                np.random.uniform(0.08, 0.25),     # start_volume
                np.random.uniform(0.08, 0.25),     # middle_volume
                np.random.uniform(0.02, 0.08),      # end_volumeï¼ˆæ˜ã‚‰ã‹ã«ä½ã„ï¼‰
                np.random.uniform(0.25, 0.6),      # end_drop_rateï¼ˆå¤§ãã„ï¼‰
                np.random.uniform(0.02, 0.08),      # last_20_percent_volume
                np.random.uniform(0.25, 0.6),      # last_20_percent_drop_rate
                np.random.uniform(1000, 2000),     # spectral_centroid_mean
                np.random.uniform(2, 4),           # speech_rate
            ]
            # ãƒã‚¤ã‚ºã‚’è¿½åŠ ï¼ˆç¾å®Ÿçš„ãªãƒãƒ©ãƒ„ã‚­ã‚’å†ç¾ï¼‰
            noise = np.random.normal(0, 0.01, len(features))
            features = np.array(features) + noise

            
            x.append(features)
            y.append("æ–‡æœ«ãŒå¼±ã„")

        # ã€Œå°å£°ã™ãã‚‹ã€éŸ³å£°ã®ãƒ‡ãƒ¼ã‚¿
        for _ in range(50): 
            features = [
                np.random.uniform(0.005, 0.04),  # mean_volumeï¼ˆå…¨ä½“çš„ã«å°ã•ã„ï¼‰
                np.random.uniform(0.005, 0.015), # std_volume
                np.random.uniform(0.005, 0.04),  # start_volume
                np.random.uniform(0.005, 0.04),  # middle_volume
                np.random.uniform(0.003, 0.025), # end_volume
                np.random.uniform(0.1, 0.35),    # end_drop_rate
                np.random.uniform(0.003, 0.025), # last_20_percent_volume
                np.random.uniform(0.1, 0.35),    # last_20_percent_drop_rate
                np.random.uniform(800, 1400),    # spectral_centroid_meanï¼ˆä½ã‚ï¼‰
                np.random.uniform(1.5, 3),       # speech_rateï¼ˆé…ã‚ï¼‰        
            ]

            # ãƒã‚¤ã‚ºã‚’è¿½åŠ 
            noise = np.random.normal(0, 0.008, len(features))  # å°‘ã—å¼±ã‚ã®ãƒã‚¤ã‚º
            features = np.array(features) + noise


            x.append(features)
            y.append("å°å£°ã™ãã‚‹")

        logger.info(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {len(x)}ã‚µãƒ³ãƒ—ãƒ«")
        return np.array(x), np.array(y)

    except Exception as e:
        logger.error(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return np.array([]), np.array([])
   
def quick_quality_assessment(features_dict):
    """è»½é‡ãªéŸ³å£°å“è³ªè©•ä¾¡ï¼ˆæ©Ÿæ¢°å­¦ç¿’ãªã—ï¼‰"""
    try:
        drop_rate = features_dict.get('end_drop_rate', 0)
        mean_volume = features_dict.get('mean_volume', 0)
        
        # ã‚ˆã‚Šè©³ç´°ãªãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è©•ä¾¡
        if drop_rate < 0.1 and mean_volume > 0.05:
            return "è‰¯å¥½", 0.95
        elif drop_rate < 0.15 and mean_volume > 0.03:
            return "è‰¯å¥½", 0.85
        elif drop_rate < 0.25 and mean_volume > 0.02:
            return "æ™®é€š", 0.7
        elif drop_rate < 0.4:
            return "æ–‡æœ«ãŒå¼±ã„", 0.6
        elif mean_volume < 0.02:
            return "å°å£°ã™ãã‚‹", 0.5
        else:
            return "ä¼šè©±ã‚’ã‚‚ã†å°‘ã—æ„è­˜ã—ã¦ã¿ã¾ã—ã‚‡ã†", 0.4
            
    except Exception as e:
        logger.error(f"å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        return "è©•ä¾¡ä¸å¯", 0.0
    
# def create_dataset_from_files(file_paths):
    # """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹é–¢æ•°ï¼ˆå°†æ¥ã®æ‹¡å¼µç”¨ï¼‰"""
    # """
    # ã“ã®é–¢æ•°ã¯å°†æ¥ã€å®Ÿéš›ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã¦
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã¾ã™ã€‚
    # ç¾åœ¨ã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã¨ã—ã¦ç©ºã®å®Ÿè£…ã«ãªã£ã¦ã„ã¾ã™ã€‚
    # """
    # try:
        # å°†æ¥ã®å®Ÿè£…:
        # 1. å„éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º
        # 2. ãƒ©ãƒ™ãƒ«ã‚’è¨­å®šï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‚„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
        # 3. ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã‚’ã¾ã¨ã‚ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
        
    #    logger.info("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã¯ä»Šå¾Œå®Ÿè£…äºˆå®šã§ã™")
    #    return np.array([]), np.array([])
        
    #except Exception as e:
    #    logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
    #    return np.array([]), np.array([])

